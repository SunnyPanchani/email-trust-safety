# Feature Engineering v2 (Industrial Security-Grade Option B)
# This file is generated by ChatGPT as an upgraded, production-ready version.
# NOTE: Replace the existing feature_engineering_v2.py with this file.

import os
import re
import gc
import json
import html
import email
import string
import numpy as np
import pandas as pd
from pathlib import Path
from email import policy
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import save_npz

import warnings
warnings.filterwarnings("ignore")

DATA_DIR = Path("data/processed")
FEATURE_DIR = Path("data/features")
FEATURE_DIR.mkdir(parents=True, exist_ok=True)

#########################################
# Utility: Clean Email Text
#########################################
def clean_text(text: str) -> str:
    if not isinstance(text, str): return ""
    text = html.unescape(text)
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

#########################################
# URL Extraction
#########################################
URL_REGEX = re.compile(r"(https?://[^\s]+)")

SUSPICIOUS_TLDS = [
    ".zip", ".xyz", ".top", ".click", ".work", ".country", ".club", ".support", ".online"
]

def extract_urls(text: str):
    if not text: return []
    return URL_REGEX.findall(text)

#########################################
# Keyword Dictionaries
#########################################
URGENT_WORDS = ["urgent", "immediately", "verify", "update", "alert", "attention"]
PRIZE_WORDS = ["winner", "lottery", "reward", "prize", "gift card"]
THREAT_WORDS = ["suspend", "deactivate", "violation", "terminate"]
MONEY_WORDS = ["bitcoin", "payment", "invoice", "bank", "fund"]

#########################################
# Feature Extraction per email
#########################################
def extract_email_features(row):
    body = clean_text(row.get("body", ""))
    subject = clean_text(row.get("subject", ""))
    from_email = str(row.get("from_email", "")).lower()

    # Body stats
    features = {
        "body_text_length": len(body),
        "body_word_count": len(body.split()),
        "body_avg_word_length": np.mean([len(w) for w in body.split()]) if body else 0,
    }

    features.update({
        "body_uppercase_ratio": sum(c.isupper() for c in body) / max(1, len(body)),
        "body_digit_ratio": sum(c.isdigit() for c in body) / max(1, len(body)),
        "body_special_char_ratio": sum(c in string.punctuation for c in body) / max(1, len(body)),
        "body_exclamation_count": body.count("!"),
        "body_question_count": body.count("?"),
        "body_consecutive_caps": len(re.findall(r"[A-Z]{3,}", body)),
    })

    # Subject stats
    features.update({
        "subject_text_length": len(subject),
        "subject_word_count": len(subject.split()),
        "subject_avg_word_length": np.mean([len(w) for w in subject.split()]) if subject else 0,
        "subject_uppercase_ratio": sum(c.isupper() for c in subject) / max(1, len(subject)),
        "subject_digit_ratio": sum(c.isdigit() for c in subject) / max(1, len(subject)),
        "subject_exclamation_count": subject.count("!"),
        "subject_consecutive_caps": len(re.findall(r"[A-Z]{3,}", subject)),
    })

    # URLs
    urls = extract_urls(body)
    features.update({
        "url_count": len(urls),
        "has_url": int(len(urls) > 0),
    })

    if urls:
        suspicious_tld = any(u.lower().endswith(tuple(SUSPICIOUS_TLDS)) for u in urls)
        ip_in_url = any(re.search(r"https?://\d+\.\d+\.\d+\.\d+", u) for u in urls)
        https_ratio = sum(u.startswith("https") for u in urls) / len(urls)
        avg_len = np.mean([len(u) for u in urls])
    else:
        suspicious_tld = False
        ip_in_url = False
        https_ratio = 0
        avg_len = 0

    features.update({
        "suspicious_tld": int(suspicious_tld),
        "ip_in_url": int(ip_in_url),
        "https_ratio": https_ratio,
        "url_length_avg": avg_len,
    })

    # Keywords
    body_lower = body.lower()
    features.update({
        "body_urgent_keyword_count": sum(k in body_lower for k in URGENT_WORDS),
        "body_prize_keyword_count": sum(k in body_lower for k in PRIZE_WORDS),
        "body_threat_keyword_count": sum(k in body_lower for k in THREAT_WORDS),
        "body_money_keyword_count": sum(k in body_lower for k in MONEY_WORDS),
    })

    # Sender features
    free_domains = ["gmail.com", "yahoo.com", "hotmail.com", "outlook.com"]
    sender_domain = from_email.split("@")[-1] if "@" in from_email else ""

    features.update({
        "free_email_provider": int(sender_domain in free_domains),
        "suspicious_sender": int(any(x in from_email for x in ["noreply-", "secure", "account", "support"])),
    })

    return features

###############################################
# TF-IDF Fitting
###############################################
def fit_tfidf(train_df):
    body_vec = TfidfVectorizer(max_features=5000, stop_words="english")
    subj_vec = TfidfVectorizer(max_features=1000, stop_words="english")

    body_vec.fit(train_df["body"].astype(str))
    subj_vec.fit(train_df["subject"].astype(str))

    return body_vec, subj_vec

###############################################
# Reputation Features
###############################################
def build_sender_reputation(train_df):
    rep = train_df.groupby("from_email")["label"].value_counts(normalize=True).unstack().fillna(0)
    rep.columns = [f"sender_label_{c}" for c in rep.columns]
    return rep

###############################################
# Main Extraction Runner
###############################################
def extract_features(df):
    rows = []
    for _, row in df.iterrows():
        rows.append(extract_email_features(row))
    return pd.DataFrame(rows)

###############################################
# MAIN EXECUTION
###############################################
if __name__ == "__main__":
    print("ðŸš€ Starting Industrial Feature Engineering (Option B)...")

    train = pd.read_parquet(DATA_DIR / "train.parquet")
    val = pd.read_parquet(DATA_DIR / "val.parquet")
    test = pd.read_parquet(DATA_DIR / "test.parquet")

    print("ðŸ“¥ Extracting metadata features...")
    metadata_train = extract_features(train)
    metadata_val = extract_features(val)
    metadata_test = extract_features(test)

    metadata_train.to_parquet(FEATURE_DIR / "metadata_train_v2.parquet")
    metadata_val.to_parquet(FEATURE_DIR / "metadata_val_v2.parquet")
    metadata_test.to_parquet(FEATURE_DIR / "metadata_test_v2.parquet")

    print("ðŸ“¥ Building sender reputation DB...")
    rep_db = build_sender_reputation(train)
    rep_db.to_parquet(FEATURE_DIR / "sender_reputation_v2.parquet")

    def add_rep(df):
        df2 = df.join(rep_db, on="from_email", how="left")
        return df2.fillna(0)

    rep_train = add_rep(train)
    rep_val = add_rep(val)
    rep_test = add_rep(test)

    rep_train.to_parquet(FEATURE_DIR / "reputation_train_v2.parquet")
    rep_val.to_parquet(FEATURE_DIR / "reputation_val_v2.parquet")
    rep_test.to_parquet(FEATURE_DIR / "reputation_test_v2.parquet")

    print("ðŸ§  Fitting TF-IDF vectorizers...")
    body_vec, subj_vec = fit_tfidf(train)

    import joblib
    joblib.dump(body_vec, FEATURE_DIR / "tfidf_body_v2.pkl")
